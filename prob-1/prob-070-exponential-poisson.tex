\documentclass{article}
\usepackage[margin=2in]{geometry}
\usepackage{graphicx}
\usepackage[toc,page]{appendix}
\usepackage{hyperref}
\usepackage{amsmath,amsfonts,amssymb,mathrsfs,amsthm}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{subcaption}
%\usepackage{titlesec}

%\setcounter{secnumdepth}{4}

%\titleformat{\paragraph}
%{\normalfont\smallsize\bfseries}{\theparagraph}{1em}{}
%\titlespacing*{\paragraph}
%{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\DeclareMathOperator*{\argmin}{arg\,min}  

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}
\usepackage{courier}
\usepackage[parfill]{parskip}


\newcommand*\conj[1]{\overline{#1}}
\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}

\newcommand\TheSolution{
  \mbox{}\par\nobreak
  \noindent
  \textbf{Solution:}\\
}

\newtheorem{theorem}{Theorem}
\newtheorem{case}{Case}
\newtheorem{lemma}{Lemma}
\newtheorem{exmp}{Example}

%\theorempostwork{\setcounter{case}{0}}

\makeatletter
\@addtoreset{case}{theorem}
\@addtoreset{case}{lemma}
\makeatother

% (Don't reset lemmas)
%\makeatletter
%\@addtoreset{lemma}{theorem}
%\makeatother

\setcounter{secnumdepth}{0}


\title{Tales of Probability, Part 7: Poisson Processes}
\author{Kevin Yuh}

\begin{document}
\maketitle


% need a better title for this section
\section{Introduction}
% subtitle: There's always a chance!

The defining feature of a Poisson process: At any moment, there is always a chance of the event of interest occurring (call it $\lambda$). This $\lambda$ is our expected rate, and for any time interval $t$, we expect $\lambda t$ events to occur.

% TODO: Figure out how to indent and space this properly
\begin{exmp}
Suppose cars drive past at an average rate of $\lambda = 3$ cars/min. In a time interval of $t = 2$ mins, we expect $\lambda t = 6$ cars to drive by. 
\end{exmp}

But let's say we are interested not in the average number of events we see, but the probability of seeing a certain number of them. (For example, what is the chance we see 8 cars in that time interval?) We intuitively expect the likelihood of seeing way fewer or way more events than the average to be pretty low - in the above example, seeing only 1 car, or seeing 30 cars, would be pretty unlikely. 

But can we derive a function that will give us these probabilities?


% Maybe proceed to derive the definitions, suggest what they are. Then in the formal definitions sections, say we already did 1 -> 2 and 2 -> 3. Then give proofs for lots of other things (explain that not all of these are necessary, but give insight). Some directions have multiple methods of proof that might be interesting. (For example, 1 -> 2 could be slightly tweaked to not use the o(h) notation?)

\section{(some cool title)}

Just to get some terms straight: For some time interval of length $t$, let $N(t)$ be the random variable corresponding to the number of events we see in that interval. We will ultimately be interested in determining $\mathbb{P}\left[N(t) = n\right]$ for all nonnegative integers $n$. 

If we are considering the number of events between two points in time (say, the time interval $\left(t_1, t_2\right]$), we will denote it $N(t_1, t_2)$. Note that all time intervals will be independent of each other, so this will simply be equal to $N(t_2 - t_1)$. 

The way we will derive it here is a bit long, but hopefully reasonably enlightening. 

In the process, we will actually, answer another question: What is the distribution of interarrival times between my events? In other words, using our example above: Suppose I just saw a car. What is the probability distribution that describes when I will see the next one?


\section{Definition and Proofs}

\subsection{Defining a Poisson process}

The Poisson process can be defined in any of the following ways:

% from netlab.tkk.fi
\begin{enumerate}
\item In any small time interval $h$, there will be either 0 or 1 events. The latter occurs with probability $\lambda h$, where $\lambda$ is the average event rate. \footnote{It intuitively makes sense to exclude more than one event happening when the time interval is very small. If we take the occurrence to have the same such probability $\lambda h$ under any circumstance, then they are independent, and the probability of having $n$ events occur in that window (call it $P_n(h)$) is $(\lambda h)^n$. 

Then as $h \rightarrow 0$, we have that $P_n(h) \in o(h)$ if $n > 1$, so it is far too small of a quantity to be meaningful.} 
\item For $n = 0, 1, 2, ...$, we have $\mathbb{P}[N(t) = n] = \frac{(\lambda t)^n}{n!}e^{-\lambda t}$, where $N(t)$ is the number of arrivals in a given time interval of length $t$. (We call this probability function the Poisson distribution.)
\item $\mathbb{P}[T > t] = e^{-\lambda t}$, where $T$ is the random variable corresponding to an interarrival time. \footnote{Equivalently, $\mathbb{P}[T < t] = 1 - e^{-\lambda t}$. This is a CDF, and the PDF is given by $\frac{d}{dt}(1 - e^{-\lambda t}) = \lambda e^{-\lambda t}$}
% clarify #3 better
\end{enumerate}


Definition 1 is perhaps the most "intuitive" physically. 

We show these definitions to be equivalent below. Note that technically, we don't need all of the proofs below. (For example, proving $1 \Rightarrow 2$, $2 \Rightarrow 3$, and $3 \Rightarrow 1$ would be sufficient. Proving $3 \Rightarrow 2$, $2 \Rightarrow 1$, and $1 \Rightarrow 3$ would also be sufficient.) 


\subsection{Proving $1 \Rightarrow 2$}

% power series are unique, see https://www.math.brown.edu/~pflueger/math1b/Lecture23.pdf

To prove this claim, we will use \textbf{generating functions}. They are essentially a nice way of "hanging coefficients on a clothesline", and allows easy manipulation of lots of related values using the properties of power series (in particular, the fact that series representations of analytic functions are unique - therefore, finding the function means finding all of the correct coefficients).
% add some references here, maybe have a document on complex analysis and power series? (I sort of started one already)

Here, our generating function will be defined as 

\begin{equation}
G_t(z) = \sum_{n=0}^\infty \mathbb{P}[N(t) = n] z^n
\end{equation}

Note that we'll have a different generating function for every time interval of length $t$. In fact, we'll write $G(t, z)$ instead of $G_t(z)$; it'll be important later.

Note that, using the very definition of expected value, we can completely validly rewrite the generating function as 

\begin{equation}
G(t, z) = \mathbb{E}[z^{N(t)}]
\end{equation}

Equivalently for any values $t_1$ and $t_2$ such that $t_2 - t_1 = t$: \footnote{Remember, this only works because of independence}

\begin{equation}
G(t, z) = \mathbb{E}[z^{N(t_1, t_2)}]
\end{equation}


Now, suppose I have a very small time interval $h$, and I want to find some information about the probability of arrival in time interval $\left(0, t+h\right]$ \textit{in terms of} that information for the time interval $\left(0, t\right]$. Let's use our generating functions! Using a bit of algebraic manipulation, we see that:


% some better explanations needed
\begin{align*}
G(t+h,z) &= \mathbb{E}[z^{N(0, t+h)}] \\
&= \mathbb{E}[z^{N(0, t) + N(t, t+h)}] && \text{Intuitively} \\
&= \mathbb{E}[z^{N(0, t)} z^{N(t, t+h)}] \\
&= \mathbb{E}[z^{N(0, t)}]  \mathbb{E}[z^{N(t, t+h)}] && \text{Independence} \\
&= G(t,z) G(h,z)
\end{align*}

% The above proof would also be a lot shorter just using some standard results.

From statement 1, when $h$ is small, we have $\mathbb{P}[N(h) = 1] = \lambda h$, and $\mathbb{P}[N(h) = 0] = 1 - \lambda h$. 

\begin{equation}
G(h,z) = (1 - \lambda h) + (\lambda h)z
\end{equation}

% TODO: these parentheses are hard to read, fix
Going back, we then have 
\begin{align*}
G(t+h,z) &= ((1 - \lambda h) + (\lambda h)z) G_t(z) \\
&= G(t,z) - \lambda h(1-z) G(t,z)
\end{align*}

and therefore

\begin{align*}
\frac{G(t+h,z) - G(t,z)}{h} &= \lambda (1-z) G(t,z)
\end{align*}

and as $h -> 0$, we get the differential equation 

\begin{align*}
\frac{dG(t,z)}{dt} &= \lambda (1-z) G(t,z)
\end{align*}

% initial values are important here!

Solving this ODE gives 

\begin{align*}
G(t,z) &= e^{\lambda t (z-1)}
\end{align*}

% rewrite proof to use t as explicit parameter

% could prove this by starting from the series and show that it evaluates to it, or just derive the coefficients here

So what did we do here? In order to get information about $G(t,z)$, we used a trick to get information about its derivative with respect to time, in order to get a useful differential equation. 

% coefficient calculation: https://en.wikipedia.org/wiki/Power_series#Analytic_functions
So now what? We figure out the coefficients in the power of series of this function. Note that our original power series setup was centered around $z_0 = 0$. Hence, the coefficient for each $z^n$ term is given by

\begin{align*}
a_n &= \left.\frac{1}{n!} \frac{d^n G(t, z)}{dz^n}\right\vert_{z=0} \\
&= \left.\frac{1}{n!} \left((\lambda t)^n e^{\lambda t (z-1)}\right) \right\vert_{z=0} \\
&= \frac{(\lambda t)^n}{n!}e^{-\lambda t}
\end{align*}

$\square$


\subsection{Proving $3 \Rightarrow 2$}

We prove this claim by induction. 

First consider the probability that, in some time interval $\left(0, z\right]$, no events occur.\footnote{Recall that we do not include 0; for a hint at why, see the footnote to Claim 1.} This is equivalent to the probability that, after an event 

TODO
% needs more rigor





\section{Interesting results}

% Calculate expected value of Poisson distribution, and discuss how this is intuitively true (the Poisson rate is the average rate). 

\section{Non-time applications}
% Bring up an example like the distribution that occurs in ACM 116, problem 5-2





\end{document}



% References: (TODO: Format them properly)

% https://www.netlab.tkk.fi/opetus/s383143/kalvot/E_poisson.pdf
